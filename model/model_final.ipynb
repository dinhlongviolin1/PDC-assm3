{"cells":[{"metadata":{},"cell_type":"markdown","source":"# ASSIGNMENT 3"},{"metadata":{},"cell_type":"markdown","source":"In this kernel, I try to make predictions where the prediction task is to determine whether a person makes over 50K a year. I implement Random Forest Classification with Python and Scikit-Learn. So, to answer the question, I build a Random Forest classifier to predict whether a person makes over 50K a year.\n\nI have used the Income classification data set for this project."},{"metadata":{},"cell_type":"markdown","source":"## Import libraries"},{"metadata":{"trusted":false},"cell_type":"code","source":"#dataframe\nimport pandas as pd\nimport numpy as np\n\n#visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#preprocessing\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import Binarizer\n\n#Classification Algorithms\nimport lightgbm as lgb\nfrom scipy import stats\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score,train_test_split, GridSearchCV, StratifiedKFold\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import svm\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve, accuracy_score\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.pipeline import Pipeline\n\n# suppressing warnings\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Import dataset"},{"metadata":{"trusted":false},"cell_type":"code","source":"X_test = pd.read_csv('test.csv')\nX_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"X_train = pd.read_csv('train.csv')\nX_train","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Exploratory data analysis"},{"metadata":{},"cell_type":"markdown","source":"* Now, I will explore the data to gain insights about the data"},{"metadata":{"trusted":false},"cell_type":"code","source":"# print the shape\nprint('The shape of the dataset : ', X_train.shape)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that there are 43957 instances and 15 attributes in the data set."},{"metadata":{"trusted":false},"cell_type":"code","source":"#Listing Dataframe columns\nX_train.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Rename column names"},{"metadata":{},"cell_type":"markdown","source":"We can see that the dataset does not have proper column names. The column names contain underscore. We should give proper names to the columns. I will do it as follows:"},{"metadata":{},"cell_type":"markdown","source":"* View summary of dataset"},{"metadata":{"trusted":false},"cell_type":"code","source":"#Determining dataframe columns characteristics\nX_train.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"X_train.head(5)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Findings\nWe can see that the dataset contains 9 character variables and 6 numerical variables.\n\nincome is the target variable.\n\nThere are no missing values in the dataset. I will explore this later."},{"metadata":{},"cell_type":"markdown","source":"* View statistical properties of dataset"},{"metadata":{"trusted":false},"cell_type":"code","source":"#Descriptive Analytics of quantitative variables in the dataframe\nX_train.describe().T","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We can see that the above df.describe().T command presents statistical properties in horizontal form."},{"metadata":{"trusted":false},"cell_type":"code","source":"#calculating mode for categorical variables:\ndef mode(list_of_nums):\n    average = list_of_nums.mode()\n    return average\n\ncategorical_var = list(X_train.select_dtypes(include=object).columns)\n\n#printing mode of categorical variables\nfor item in categorical_var:\n    print(item + \" mode: \"+ mode(X_train[item]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Checking if any cells has missing values\nX_train.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Interpretation\nWe can see that there are 3 columns missing values in the dataset."},{"metadata":{"trusted":false},"cell_type":"code","source":"## Dealing with NaN value\nmissing_val_col = ['workclass','occupation','native-country']\n\nfor i in missing_val_col:\n    X_train[i] = X_train[i].fillna(method='ffill')\nX_train.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Remove white space"},{"metadata":{"trusted":false},"cell_type":"code","source":"X_train.columns = X_train.columns.str.lstrip()\nX_test.columns = X_test.columns.str.lstrip()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Explore Categorical Variables"},{"metadata":{},"cell_type":"markdown","source":"### Find categorical variables"},{"metadata":{"trusted":false},"cell_type":"code","source":"categorical = [var for var in X_train.columns if X_train[var].dtype=='O']\n\nprint('There are {} categorical variables\\n'.format(len(categorical)))\n\nprint('The categorical variables are :\\n\\n', categorical)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"X_train[categorical].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Frequency distribution of categorical variables"},{"metadata":{},"cell_type":"markdown","source":"Now, we will check the frequency distribution of categorical variables."},{"metadata":{"trusted":false},"cell_type":"code","source":"for var in categorical: \n    \n    print(X_train[var].value_counts())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Percentage of frequency distribution of values"},{"metadata":{"trusted":false},"cell_type":"code","source":"for var in categorical:\n    \n     print(X_train[var].value_counts()/np.float(len(X_train)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# view the unique values\n\nX_train['income_>50K'].unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Checking the distribution of target variable\ndisplay(X_train['income'].value_counts(), X_train['income'].value_counts(normalize = True)*100)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Visualize income variable"},{"metadata":{"trusted":false},"cell_type":"code","source":"# visualize frequency distribution of income variable\n\nf,ax=plt.subplots(1,2,figsize=(18,8))\n\nax[0] = X_train['income'].value_counts().plot.pie(explode=[0,0],autopct='%1.1f%%',ax=ax[0],shadow=True)\nax[0].set_title('Income Share')\n\n\n#f, ax = plt.subplots(figsize=(6, 8))\nax[1] = sns.countplot(x=\"income\", data=X_train, palette=\"Set1\")\nax[1].set_title(\"Frequency distribution of income variable\")\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Visualize income with gender variable"},{"metadata":{"trusted":false},"cell_type":"code","source":"f, ax = plt.subplots(figsize=(10, 8))\nax = sns.countplot(x=\"income\", hue=\"gender\", data=X_train, palette=\"Set1\")\nax.set_title(\"Frequency distribution of income variable with gender\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Interpretation:\nWe can see that males make more money than females in both the income categories."},{"metadata":{},"cell_type":"markdown","source":"Visualize income with race"},{"metadata":{"trusted":false},"cell_type":"code","source":"f, ax = plt.subplots(figsize=(10, 8))\nax = sns.countplot(x=\"income\", hue=\"race\", data=X_train, palette=\"Set1\")\nax.set_title(\"Frequency distribution of income variable with race\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Interpretation¶\n* We can see that whites make more money than non-whites in both the income categories."},{"metadata":{},"cell_type":"markdown","source":"Explore workclass variable"},{"metadata":{"trusted":false},"cell_type":"code","source":"# view the unique labels\n\nX_train.workclass.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# view frequency distribution of values\n\nX_train.workclass.value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Visualize workclass variable"},{"metadata":{"trusted":false},"cell_type":"code","source":"f, ax = plt.subplots(figsize=(10, 6))\nax = X_train.workclass.value_counts().plot(kind=\"bar\", color=\"green\")\nax.set_title(\"Frequency distribution of workclass variable\")\nax.set_xticklabels(X_train.workclass.value_counts().index, rotation=30)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Interpretation¶\n* We can see that there are lot more private workers than other category of workers."},{"metadata":{},"cell_type":"markdown","source":"Visualize workclass variable with income variable"},{"metadata":{"trusted":false},"cell_type":"code","source":"f, ax = plt.subplots(figsize=(12, 8))\nax = sns.countplot(x=\"workclass\", hue=\"income\", data=X_train, palette=\"Set1\")\nax.set_title(\"Frequency distribution of workclass variable wrt income\")\nax.legend(loc='upper right')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Interpretation\n* We can see that workers make less than equal to 50k in most of the working categories.\n\n* But this trend is more appealing in Private workclass category."},{"metadata":{},"cell_type":"markdown","source":"Visualize workclass variable with education variable"},{"metadata":{"trusted":false},"cell_type":"code","source":"f, ax = plt.subplots(figsize=(12, 8))\nax = sns.countplot(x=\"workclass\", hue=\"education\", data=X_train, palette=\"Set1\")\nax.set_title(\"Frequency distribution of workclass variable with education\")\nax.legend(loc='upper right')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Explore occupation variable"},{"metadata":{"trusted":false},"cell_type":"code","source":"# view unique labels\nX_train.occupation.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#check the frequency distribution of values\n\nX_train.occupation.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# visualize frequency distribution of `occupation` variable\n\nf, ax = plt.subplots(figsize=(12, 8))\nax = sns.countplot(x=\"occupation\", data=X_train, palette=\"Set1\")\nax.set_title(\"Frequency distribution of occupation variable\")\nax.set_xticklabels(X_train.occupation.value_counts().index, rotation=30)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Explore native_country variable"},{"metadata":{"trusted":false},"cell_type":"code","source":"# view unique labels \n\nX_train.native_country.unique()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# check frequency distribution of values\n\nX_train.native_country.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# visualize frequency distribution of `native_country` variable\n\nf, ax = plt.subplots(figsize=(16, 12))\nax = sns.countplot(x=\"native_country\", data=X_train, palette=\"Set1\")\nax.set_title(\"Frequency distribution of native_country variable\")\nax.set_xticklabels(X_train.native_country.value_counts().index, rotation=90)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# check for cardinality in categorical variables\n\nfor var in categorical:\n    \n    print(var, ' contains ', len(X_train[var].unique()), ' labels')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Explore Numerical Variables"},{"metadata":{},"cell_type":"markdown","source":"* Find numerical variables"},{"metadata":{"trusted":false},"cell_type":"code","source":"numerical = [var for var in X_train.columns if X_train[var].dtype!='O']\n\nprint('There are {} numerical variables\\n'.format(len(numerical)))\n\nprint('The numerical variables are :\\n\\n', numerical)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"X_train[numerical].head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Summary of numerical variables¶\n* There are 6 numerical variables.\n\n* These are given by age, fnlwgt, education_num,capital_gain, capital_loss and hours_per_week.\n\n* All of the numerical variables are of discrete data type."},{"metadata":{},"cell_type":"markdown","source":"Explore numerical variables"},{"metadata":{},"cell_type":"markdown","source":"Explore age variable"},{"metadata":{"trusted":false},"cell_type":"code","source":"f, ax = plt.subplots(figsize=(10,8))\nx = X_train['age']\nax = sns.distplot(x, bins=10, color='blue')\nax.set_title(\"Distribution of age variable\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Detect outliers in age variable with boxplot"},{"metadata":{"trusted":false},"cell_type":"code","source":"f, ax = plt.subplots(figsize=(10,8))\nx = X_train['age']\nax = sns.boxplot(x)\nax.set_title(\"Visualize outliers in age variable\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Explore relationship between age and income variables"},{"metadata":{"trusted":false},"cell_type":"code","source":"f, ax = plt.subplots(figsize=(10, 8))\nax = sns.boxplot(x=\"income\", y=\"age\", data=X_train)\nax.set_title(\"Visualize income wrt age variable\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Interpretation¶\n* As expected, younger people make less money as compared to senior people."},{"metadata":{},"cell_type":"markdown","source":"Visualize income with age and sex variable"},{"metadata":{"trusted":false},"cell_type":"code","source":"f, ax = plt.subplots(figsize=(10, 8))\nax = sns.boxplot(x=\"income\", y=\"age\", hue=\"gender\", data=X_train)\nax.set_title(\"Visualize income with age and sex variable\")\nax.legend(loc='upper right')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Interpretation\n* Senior people make more money than younger people."},{"metadata":{},"cell_type":"markdown","source":"Visualize relationship between race and age"},{"metadata":{"trusted":false},"cell_type":"code","source":"plt.figure(figsize=(12,8))\nsns.boxplot(x ='race', y=\"age\", data = X_train)\nplt.title(\"Visualize age wrt race\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Interpretation¶\n* Whites are more older than other groups of people."},{"metadata":{},"cell_type":"markdown","source":"Find out the correlations"},{"metadata":{"trusted":false},"cell_type":"code","source":"# plot correlation heatmap to find out correlations\n\nX_train.corr().style.format(\"{:.4}\").background_gradient(cmap=plt.get_cmap('coolwarm'), axis=1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Interpretation\n* We can see that there is no strong correlation between variables."},{"metadata":{"trusted":false},"cell_type":"code","source":"sns.pairplot(X_train, hue=\"income\")\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# check the shape of X_train and X_test\n\nX_train.shape, X_test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Feature Engineering"},{"metadata":{},"cell_type":"markdown","source":"#### Display categorical variables in training set"},{"metadata":{"trusted":false},"cell_type":"code","source":"ord_features = []\n\n#categorical features\ncat_features = list(X_train.select_dtypes(include=['object']).drop(ord_features, axis=1))\n\n#numeric features\nnum_features = list(X_train.select_dtypes(include=['number']))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Apply label encoder to each column with categorical data\nlabel_encoder = LabelEncoder()\nfor col in ord_features:\n    X_train[col] = label_encoder.fit_transform(X_train[col])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Apply one-hot encoder to each column with categorical data\nOH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\nOH_X_train = pd.DataFrame(OH_encoder.fit_transform(X_train[cat_features]))\nOH_X_test = pd.DataFrame(OH_encoder.fit_transform(X_test[cat_features]))\n\n# One-hot encoding removed index; put it back\nOH_X_train.index = X_train.index\nOH_X_test.index = X_test.index\n\n# Remove categorical columns (will replace with one-hot encoding)\nnum_col_X_train = X_train.drop(cat_features, axis=1)\nnum_col_X_test = X_test.drop(cat_features, axis=1)\n\n# Add one-hot encoded columns to numerical features\nOH_X_train_finish = pd.concat([num_col_X_train, OH_X_train], axis=1)\nOH_X_test_finish = pd.concat([num_col_X_test, OH_X_test], axis=1)\n\nOH_X_train_finish","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Display numerical variables in training set"},{"metadata":{},"cell_type":"markdown","source":"#### Encode categorical variables"},{"metadata":{},"cell_type":"markdown","source":"### Declare feature vector and target variable"},{"metadata":{"trusted":false},"cell_type":"code","source":"X=OH_X_train_finish.drop(['income_>50K'],axis=1)\ny=OH_X_train_finish[['income_>50K']]\n\nfrom collections import Counter\nprint(sorted(Counter(y).items()))","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.datasets import make_classification\nX, y = make_classification(n_samples=21000, n_features=13, n_informative=2,\n                            n_redundant=0, n_repeated=0, n_classes=2,\n                            n_clusters_per_class=1,\n#                             weights=[0.01, 0.05, 0.94],\n                            class_sep=0.8, random_state=0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"from imblearn.under_sampling import ClusterCentroids\ncc = ClusterCentroids(random_state=0)\nX_resampled, Y_resampled = cc.fit_resample(X, y)\nprint(sorted(Counter(Y_resampled).items()))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Split data into separate training and test set"},{"metadata":{"trusted":false},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# X_test[categorical_cols] = X_test[categorical_cols].apply(lambda col: le.fit_transform(col))\n# X_test[categorical_cols].head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"X_train.shape\nX_test.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Modelling"},{"metadata":{"trusted":false},"cell_type":"code","source":"def train_and_plot(model, X_train, y_train, X_test, y_test):\n    model.fit(X_train, y_train)\n\n    print('Model accuracy for train set: {0:.3f}'.format(model.score(X_train, y_train)))\n    print('Model accuracy for test set: {0:.3f}'.format(model.score(X_test, y_test)))\n\n    y_pred = model.predict(X_test)\n\n    # Classification Report\n    print('\\n{}'.format(classification_report(y_test, y_pred)))\n\n    # Confusion Matrix\n    cm = confusion_matrix(y_test, y_pred)\n    print('\\nConfusion Matrix:\\n', cm)\n\n    # Accuracy Score\n    auc = accuracy_score(y_test, y_pred)\n    print('\\nAccuracy Score: ', auc.round(3))\n\n    # ROC Curve\n    model_roc_auc = roc_auc_score(y_test, model.predict(X_test))\n    fpr, tpr, thresholds = roc_curve(y_test, model.predict_proba(X_test)[:,1])\n    plt.figure(figsize = (12.8 , 6))\n    plt.plot(fpr, tpr, label = 'AUC = {})'.\\\n             format(model_roc_auc.round(4)))\n    plt.plot([0, 1], [0, 1],'r--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic')\n    plt.legend(loc = 'lower right')\n    plt.show()\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Importing necessary libraries\nfrom bayes_opt import BayesianOptimization\nimport xgboost as xgb\n\n#Converting the dataframe into XGBoost’s Dmatrix object\ndtrain = xgb.DMatrix(X,y)\n\n#Bayesian Optimization function for xgboost\n#specify the parameters you want to tune as keyword arguments\ndef bo_tune_xgb(max_depth, gamma, n_estimators ,learning_rate):\n    params = {'max_depth': int(max_depth),\n              'gamma': gamma,\n              'n_estimators': int(n_estimators),\n              'learning_rate':learning_rate,\n              'eta': 0.1,\n              'eval_metric': 'rmse'}\n    #Cross validating with the specified parameters in 5 folds and 70 iterations\n    cv_result = xgb.cv(params, dtrain, num_boost_round=70, nfold=5)\n    #Return the negative RMSE\n    return -1.0 * cv_result['test-rmse-mean'].iloc[-1]\n\n#Invoking the Bayesian Optimizer with the specified parameters to tune\nxgb_bo = BayesianOptimization(bo_tune_xgb, {'max_depth': (3, 10),\n                                             'gamma': (0, 1),\n                                             'learning_rate':(0,1),\n                                             'n_estimators':(100,120)\n                                            })\n\n#performing Bayesian optimization for 5 iterations with 8 steps of random exploration with an #acquisition function of expected improvement\nxgb_bo.maximize(n_iter=5, init_points=8, acq='ei')","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Extracting the best parameters\nparams = xgb_bo.max['params']\nprint(params)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"#Converting the max_depth and n_estimator values from float to int\nparams['max_depth']= int(params['max_depth'])\nparams['n_estimators']= int(params['n_estimators'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# params['gamma']= float(params['gamma'])\n# params['learning_rate']= float(params['learning_rate'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# XGBOOST\nimport xgboost\nxgb1 = xgboost.XGBClassifier(gamma=0.8,\n                           learning_rate=0.2,\n                           max_depth=8,\n                           n_estimators=119)\nmodel9 = train_and_plot(xgb1, X_train, y_train, X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# XGBOOST\nimport xgboost\nXGB = xgboost.XGBClassifier()\nmodel1 = train_and_plot(XGB, X_train, y_train, X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Logistic Regression\nlr = LogisticRegression(random_state=0, solver='lbfgs', multi_class='ovr') # creates a lR instance\nmodel2 = train_and_plot(lr, X_train, y_train, X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# hyperparameter tuning\nLR = LogisticRegression()\n\nparams = {'penalty': ['l1', 'l2'], 'C': [0.001, 0.01, 0.1, 1, 10, 100], 'max_iter': [100, 110, 120, 130, 140]}\n\nskf = StratifiedKFold(n_splits = 10)\nLR_hyper = GridSearchCV(LR, param_grid = params, n_jobs = -1, cv = skf)\n\nmodel3 = train_and_plot(LR_hyper, X_train, y_train, X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Decision Trees\ndt = DecisionTreeClassifier(criterion = 'gini', splitter='best', max_depth=8)\nmodel4 = train_and_plot(dt, X_train, y_train, X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Random Forests \nrf = RandomForestClassifier(n_estimators=1000, max_depth=20, random_state=1000)\nmodel5 = train_and_plot(rf, X_train, y_train, X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# K-NN\nknn = KNeighborsClassifier(n_neighbors = 25)\nmodel6 = train_and_plot(knn, X_train, y_train, X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Import the model we are using\nfrom sklearn.ensemble import GradientBoostingClassifier\n# Instantiate model with 1000 decision trees\ngbm = GradientBoostingClassifier(n_estimators = 119, \n                                 learning_rate = 0.05, \n                                 random_state = 1000, \n                                 n_iter_no_change = 5, \n                                 max_depth=8, \n                                 min_samples_split=3, \n                                 min_samples_leaf=3,\n                                 verbose=1)\n# Train the model on training data\nmodel8 = train_and_plot(gbm, X_train, y_train, X_test, y_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# computes the confusion matrix and performence indicators\ndef get_performances(actual_Y, pred_Y):\n    cm = confusion_matrix(actual_Y, pred_Y.round())\n    total = sum(sum(cm))\n    accuracy = (cm[0,0]+cm[1,1])/total\n    return accuracy","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"pred_Y_xgbh = xgb1.predict(X_test) # predicts the Y given the fitted model XGBoost\n\npred_Y_xgb = XGB.predict(X_test) # predicts the Y given the fitted model XGBoost\n\npred_Y_lr = lr.predict(X_test) # predicts the Y given the fitted model lr\n\npred_Y_lrh = LR_hyper.predict(X_test) # predicts the Y given the fitted model lr with hypertunning\n\npred_Y_dt = dt.predict(X_test) # predicts the Y given the fitted model dt\n\npred_Y_rf = rf.predict(X_test) # predicts the Y given the fitted model rf\n\npred_Y_knn = knn.predict(X_test) # predicts the Y given the fitted model knn\n\npred_Y_gbm = gbm.predict(X_test) # predicts the Y given the fitted model Gradient Boosting","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# accuracy, sensitivity, and specificity for model XGboost\naccuracy_xgbh = get_performances(y_test, pred_Y_xgbh)\n\n# accuracy, sensitivity, and specificity for model XGboost\naccuracy_xgb = get_performances(y_test, pred_Y_xgb)\n\n# accuracy, sensitivity, and specificity for model lr\naccuracy_lr = get_performances(y_test, pred_Y_lr)\n\n# accuracy, sensitivity, and specificity for model lr\naccuracy_lrh = get_performances(y_test, pred_Y_lrh)\n\n# accuracy, sensitivity, and specificity for model dt\naccuracy_dt = get_performances(y_test, pred_Y_dt)\n\n# accuracy, sensitivity, and specificity for model rf\naccuracy_rf = get_performances(y_test, pred_Y_rf)\n\n# accuracy, sensitivity, and specificity for model knn\naccuracy_knn = get_performances(y_test, pred_Y_knn)\n\n# accuracy, sensitivity, and specificity for model Neural Networks\naccuracy_gbm = get_performances(y_test, pred_Y_gbm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# Builds a dataframe using the performance indicators so that we can compare the models easily\n\nperf = pd.DataFrame([accuracy_xgbh,accuracy_xgb,accuracy_lr, accuracy_lrh, accuracy_dt,accuracy_rf,accuracy_knn,accuracy_gbm], \n                    columns = ['accuracy'], \n                    index = ['XGBoost-Hyperparam','XGBoost','Logistic Regression','Logistic Regression Hyper',\n                             'Decision Trees', 'Random Forest','K-NN','Gradient Boosting'])\nperf","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# create the classifier with n_estimators = 100\n\nclf = RandomForestClassifier(n_estimators=100, random_state=0)\n\n# fit the model to the training set\n\nclf.fit(X_train, y_train)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Prediction"},{"metadata":{"trusted":false},"cell_type":"code","source":"y_pred = xgb1.predict(X_test)\nprint(\"Predicted values:\")\nprint(y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}